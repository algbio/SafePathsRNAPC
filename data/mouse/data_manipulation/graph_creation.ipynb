{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import of used libraries\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ariel/.pyenv/versions/3.9.6/envs/jupyter/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Load data in .bed format\n",
    "d = pd.read_csv(f'../exons.bed', header=None, sep='\\t')\n",
    "#Only strings in chromosome names\n",
    "d[0] = list(map(lambda chr_name: str(chr_name), d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It computes an artificial set of offsets for the chromosomes in the bed file\n",
    "d_pos = d[d[5] == '+']\n",
    "starts_exons = list(d_pos[10])+list(d_pos[11])\n",
    "starts_exons = list(map(lambda x: x.split(',')[:-1], starts_exons))\n",
    "import itertools\n",
    "starts_exons = list(itertools.chain.from_iterable(starts_exons))\n",
    "starts_exons = list(map(lambda x: int(x), starts_exons))\n",
    "\n",
    "relative_max_away = max(starts_exons)\n",
    "\n",
    "offset = dict()\n",
    "chr_names = sorted(list(set(d[0])))\n",
    "previous_chr_name = chr_names[0]\n",
    "offset[previous_chr_name] = 0\n",
    "for chr_name in chr_names[1:]:\n",
    "    offset[chr_name] = max(d_pos[d_pos[0]==previous_chr_name][1])+offset[previous_chr_name]+2*relative_max_away\n",
    "    previous_chr_name = chr_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bed_format(d, offset):\n",
    "    ## Get positive strand\n",
    "    d_pos = d[d[5] == '+']\n",
    "    \n",
    "    ## Drop unnecessary columns and name the required\n",
    "    d_pos = d_pos.drop([2, 4, 5, 6, 7, 8, 9], axis=1)\n",
    "    d_pos.columns = ['chromosome', 'transcript_start', 'transcript_id', 'exon_sizes', 'exon_starts']\n",
    "    \n",
    "    ##Compute absolute genome positions for transcripts\n",
    "    d_pos.transcript_start = list(map(lambda x: x[0]+offset[x[1]], list(zip(d_pos.transcript_start, d_pos.chromosome))))\n",
    "    \n",
    "    ##Maybe something different for negative strand?\n",
    "\n",
    "    #Convert exon_sizes and exon_starts into list of numbers\n",
    "    d_pos.exon_sizes = list(map(lambda x: list(map(numpy.int64 , x.split(','))) ,d_pos.exon_sizes))\n",
    "    d_pos.exon_starts = list(map(lambda x: list(map(numpy.int64 , x.split(','))) ,d_pos.exon_starts))\n",
    "    \n",
    "    return d_pos\n",
    "\n",
    "d_pos = preprocess_bed_format(d, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv(f'../CDS.bed', header=None, sep='\\t')\n",
    "s[0] = list(map(lambda chr_name: str(chr_name), s[0]))\n",
    "\n",
    "s_pos = preprocess_bed_format(s, offset)\n",
    "\n",
    "s_c = dict()\n",
    "for index, CDSs in s_pos.iterrows():\n",
    "    first_CDS_start = int(CDSs['transcript_start'])\n",
    "    s_c[CDSs['transcript_id']] = list(map(lambda x: (x[0]+first_CDS_start, x[0]+first_CDS_start+x[1]-1), zip(CDSs['exon_starts'], CDSs['exon_sizes'])))\n",
    "\n",
    "CDSs_column = list()\n",
    "\n",
    "for index, transcript in d_pos.iterrows():\n",
    "    if transcript['transcript_id'] in s_c:\n",
    "        CDSs_column.append(list(map(lambda CDS: (CDS[0], CDS[1]), s_c[transcript['transcript_id']])))\n",
    "    else:\n",
    "        CDSs_column.append(list())\n",
    "d_pos['CDSs'] = CDSs_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function transforms processed table into a list of exon_endpoints\n",
    "def get_exon_endpoint_positions(d):\n",
    "    exons_endpoints = list()\n",
    "    for index, transcript in d.iterrows():\n",
    "        start = transcript.transcript_start\n",
    "        transcript_id = transcript.transcript_id\n",
    "        transcript_exons_endpoints = list()\n",
    "        for i in range(len(transcript.exon_starts)):\n",
    "            exon_start = start+transcript.exon_starts[i]\n",
    "            transcript_exons_endpoints.append({'index':index, 'position': exon_start, 'transcript_id': transcript_id, 'exon_index': i, 'start_point': True})\n",
    "            transcript_exons_endpoints.append({'index':index, 'position': exon_start+transcript.exon_sizes[i]-1, 'transcript_id': transcript_id, 'exon_index': i, 'start_point': False})\n",
    "        exons_endpoints.append(transcript_exons_endpoints)\n",
    "    return [item for sublist in exons_endpoints for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtains a list of all exons and sorts them according to its position and in case of ties it puts first the starting positions\n",
    "exon_endpoint_pos_list = get_exon_endpoint_positions(d_pos)\n",
    "exon_endpoint_pos_list.sort(key=lambda x: [x['position'], not(x['start_point'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the exons when overlapping with another\n",
    "active_exons = dict()\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        exon_endpoint['starting_points'] = list()\n",
    "        active_exons[(exon_endpoint['index'], exon_endpoint['exon_index'])] = exon_endpoint\n",
    "        \n",
    "        for key in active_exons:\n",
    "            active_exons[key]['starting_points'].append(exon_endpoint['position'])\n",
    "    else:\n",
    "        for key in active_exons:\n",
    "            active_exons[key]['starting_points'].append(exon_endpoint['position']+1)\n",
    "        del active_exons[(exon_endpoint['index'], exon_endpoint['exon_index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute pseudo-exons\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        exon_endpoint['pseudo_exons'] = list()\n",
    "        previous_value = exon_endpoint['starting_points'][0]\n",
    "        for i in range(1, len(exon_endpoint['starting_points'])):\n",
    "            if previous_value != exon_endpoint['starting_points'][i]:\n",
    "                exon_endpoint['pseudo_exons'].append((previous_value, exon_endpoint['starting_points'][i]-1))\n",
    "                previous_value = exon_endpoint['starting_points'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group pseudo_exons by transcript (assumption: exons of a transcript do not overlap)\n",
    "transcripts = dict()\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        if transcripts.get(exon_endpoint['index'], None) is None:\n",
    "            transcripts[exon_endpoint['index']] = {'pseudo_exons': list(), 'index': exon_endpoint['index']}\n",
    "        for pseudo_exon in exon_endpoint['pseudo_exons']:\n",
    "            transcripts[exon_endpoint['index']]['pseudo_exons'].append(pseudo_exon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intersect(i1, i2):\n",
    "#     mac = min(i1[0], i2[0])\n",
    "#     Mac = max(i1[0], i2[0])\n",
    "#     mbd = min(i1[1], i2[1])\n",
    "#     return mac <= Mac and Mac <= mbd\n",
    "    \n",
    "# def get_CDS_path(pseudo_exon_path, CDS):\n",
    "#     CDS_path = list()\n",
    "#     for pseudo_exon in pseudo_exon_path:\n",
    "#         if intersect(pseudo_exon, CDS):\n",
    "#             CDS_path.append(pseudo_exon)\n",
    "#     return CDS_path\n",
    "\n",
    "# #Compute CDSs in pseudo_exons terms per each transcript\n",
    "# for transcript_id in transcripts:\n",
    "#     if transcript_id in s_c:\n",
    "#         CDSs = s_c[transcript_id]\n",
    "#         transcripts[transcript_id]['CDSs'] = list(map(lambda CDS:\n",
    "#                                                              get_CDS_path(transcripts[transcript_id]['pseudo_exons'],\n",
    "#                                                                             CDS)\n",
    "#                                                               ,CDSs))\n",
    "#     else:\n",
    "#         transcripts[transcript_id]['CDSs'] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the pseudo_exons column\n",
    "pseudo_exons_list = list(transcripts.values())\n",
    "pseudo_exons_list.sort(key= lambda x: x['index'])\n",
    "\n",
    "pseudo_exons_column = list(map(lambda x: x['pseudo_exons'] ,pseudo_exons_list))\n",
    "\n",
    "d_pos['pseudo_exons'] = pseudo_exons_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vertex set\n",
    "vertices = dict() # Given a exon presudo_exon (x,y) return its id\n",
    "vertices_inv = dict() # Given an id returns the corresponding exon (x,y)\n",
    "next_id = 0\n",
    "for pseudo_exons in pseudo_exons_column:\n",
    "    for pseudo_exon in pseudo_exons:\n",
    "        if vertices.get(pseudo_exon, None) is None:\n",
    "            vertices[pseudo_exon] = next_id\n",
    "            vertices_inv[next_id] = pseudo_exon\n",
    "            next_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(i1, i2):\n",
    "    mac = min(i1[0], i2[0])\n",
    "    Mac = max(i1[0], i2[0])\n",
    "    mbd = min(i1[1], i2[1])\n",
    "    return mac <= Mac and Mac <= mbd\n",
    "\n",
    "def get_CDS(CDSs, path, vertices_inv):\n",
    "    CDS = dict()\n",
    "    if CDSs:\n",
    "        first_coding_vertex = 0\n",
    "        while not(intersect(vertices_inv[path[first_coding_vertex]], CDSs[0])):\n",
    "            first_coding_vertex += 1\n",
    "\n",
    "        last_coding_vertex = len(path)-1\n",
    "        while not(intersect(vertices_inv[path[last_coding_vertex]], CDSs[-1])):\n",
    "            last_coding_vertex -= 1\n",
    "\n",
    "        cds_path = path[first_coding_vertex:last_coding_vertex+1]\n",
    "        \n",
    "        CDS['subpath'] = cds_path\n",
    "        CDS['start'] = CDSs[0][0]\n",
    "        CDS['end'] = CDSs[-1][1]\n",
    "        \n",
    "    return CDS\n",
    "\n",
    "# Build edge set, source and target vertices\n",
    "# It also builds the transcript paths starting at every source (This could generate multiedges in the graph)\n",
    "\n",
    "## These dicts are indexed by the pair [exon_start, exon_end]\n",
    "transcript_paths = dict()\n",
    "sources = dict()\n",
    "targets = dict()\n",
    "\n",
    "## The keys are the edges and the edges and the value the corresponding id\n",
    "edges = dict()\n",
    "next_id = 0 \n",
    "for index, row in d_pos.iterrows():\n",
    "    pseudo_exons = row['pseudo_exons']\n",
    "    if sources.get(pseudo_exons[0], None) is None:\n",
    "        sources[pseudo_exons[0]] = set()\n",
    "    if targets.get(pseudo_exons[-1], None) is None:\n",
    "        targets[pseudo_exons[-1]] = set()\n",
    "    \n",
    "    if transcript_paths.get(pseudo_exons[0], None) is None:\n",
    "        transcript_paths[pseudo_exons[0]] = list()\n",
    "    \n",
    "    \n",
    "    sources[pseudo_exons[0]].add(index)\n",
    "    targets[pseudo_exons[-1]].add(index)\n",
    "    \n",
    "    CDSs = row['CDSs']\n",
    "#     CDSs_path = row['CDSs_path']\n",
    "    \n",
    "    transcript_path = [vertices[pseudo_exons[0]]]\n",
    "    ## Consecutive pseudo exons in pseudo_exons are linked by an edge\n",
    "    for i in range(len(pseudo_exons)-1):\n",
    "        current_pe = pseudo_exons[i]\n",
    "        next_pe = pseudo_exons[i+1]\n",
    "        edge = (vertices[current_pe], vertices[next_pe])\n",
    "        if edges.get(edge, None) is None:\n",
    "            edges[edge] = next_id\n",
    "            next_id += 1\n",
    "        transcript_path.append(vertices[next_pe])\n",
    "    \n",
    "    transcript_paths[pseudo_exons[0]].append({\n",
    "        'transcript_path':transcript_path,\n",
    "        'CDS': get_CDS(list(map(lambda CDS: (int(CDS[0]), int(CDS[1])),CDSs)), transcript_path, vertices_inv)\n",
    "    })\n",
    "    \n",
    "#     transcript_paths[pseudo_exons[0]].append({\n",
    "#         'transcript_path':transcript_path,\n",
    "#         'CDSs': list(map(lambda CDS: {\n",
    "#             'CDS': (int(CDS[0][0]), int(CDS[0][1])),\n",
    "#             'path': list(map(lambda interval: vertices[interval], CDS[1]))\n",
    "#         }, zip(CDSs, CDSs_path)))\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph to find weakly connected components, and also \n",
    "# computes len, sources, target and transcipt paths for each\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(len(vertices)))\n",
    "G.add_edges_from(edges.keys())\n",
    "\n",
    "components = list()\n",
    "for component_v in nx.weakly_connected_components(G):\n",
    "    component_dict = {'graph':G.subgraph(component_v)}\n",
    "    component_dict['len'] = len(component_dict['graph'])\n",
    "    sources_component = set()\n",
    "    targets_component = set()\n",
    "    transcript_paths_component = list()\n",
    "    \n",
    "    for vertex in component_v:\n",
    "        interval = vertices_inv[vertex]\n",
    "        if sources.get(interval, None) is not None:\n",
    "            sources_component.add(vertex)\n",
    "            transcript_paths_component += transcript_paths[interval]\n",
    "            \n",
    "        if targets.get(interval, None) is not None:\n",
    "            targets_component.add(vertex)\n",
    "    component_dict['sources'] = sources_component\n",
    "    component_dict['targets'] = targets_component\n",
    "    component_dict['transcript_paths'] = transcript_paths_component\n",
    "    component_dict['vertex_constrains'] = set(component_v)\n",
    "    \n",
    "    components.append(component_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "## It stores the networkx graph, the transcript paths, the sources and targets\n",
    "def store_components_to_files(component, i):\n",
    "    gene_graph = component['graph']\n",
    "    nx.write_edgelist(gene_graph, path=f'../gene_graphs/graphs/component_{i+1}.edgelist', delimiter=':')\n",
    "    \n",
    "    transcript_paths = component['transcript_paths']\n",
    "    f = open(f'../gene_graphs/transcript_paths/component_{i+1}.json', 'w')\n",
    "    dump(transcript_paths, f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    sources = list(component['sources'])\n",
    "    f = open(f'../gene_graphs/sources/component_{i+1}.json', 'w')\n",
    "    dump(sources, f)\n",
    "    f.close()\n",
    "    \n",
    "    targets = list(component['targets'])\n",
    "    f = open(f'../gene_graphs/targets/component_{i+1}.json', 'w')\n",
    "    dump(targets, f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    vertex_constrains = list(component['vertex_constrains'])\n",
    "    f = open(f'../gene_graphs/vertex_constrains/component_{i+1}.json', 'w')\n",
    "    dump(vertex_constrains, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store components/gene_graphs to files\n",
    "for i,component in enumerate(components):\n",
    "    store_components_to_files(component, i)\n",
    "\n",
    "\n",
    "## Store vertices_inv: id --> (genome_pos, genome_pos)\n",
    "vertices_inv = {key: (int(x), int(y)) for key, (x, y) in vertices_inv.items()}\n",
    "f = open(f'../gene_graphs/vertices_inv.json', 'w')\n",
    "dump(vertices_inv, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It stores the corresponding component as lemon graph format file in filename\n",
    "## Vertex mappings original_id (networkx's id), is_source (if it is a source), is_target (if it is a target)\n",
    "## and is_vertex_constrain (if it is a vertex constrain) are included in the file\n",
    "def store_to_file_in_lemon_format_with_mappings(component, filename):\n",
    "    G = component['graph']\n",
    "    sources = component['sources']\n",
    "    targets = component['targets']\n",
    "    constrains = component['vertex_constrains']\n",
    "    \n",
    "    file = open(filename, 'w')\n",
    "    file.write(\"@nodes\\n\")\n",
    "    file.write(\"label\\toriginal_id\\tis_source\\tis_target\\tis_vertex_constrain\\t\\n\")\n",
    "    for vertex in G.nodes:\n",
    "        file.write(str(vertex)+\"\\t\"+str(vertex)+\"\\t\"+str(1 if vertex in sources else 0)+\"\\t\"+str(1 if vertex in targets else 0)+\"\\t\"+str(1 if vertex in constrains else 0)+\"\\t\\n\")\n",
    "    file.write(\"@arcs\\n\")\n",
    "    file.write(\"\\t\\tlabel\\t\\n\")\n",
    "    for i, edge in enumerate(G.edges):\n",
    "        file.write(str(edge[0])+\"\\t\"+str(edge[1])+\"\\t\"+ str(i) + \"\\t\\n\")  \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store components with more than to vertices to the corresponding lemon graph format\n",
    "## Can be skipped if already computed\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2 and len(component['transcript_paths']) > 1:\n",
    "        store_to_file_in_lemon_format_with_mappings(component,'../lgf/component_'+str(i+1)+'.lgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = open('../gene_graphs/info','w')\n",
    "info.write(str(len(components)))\n",
    "info.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
