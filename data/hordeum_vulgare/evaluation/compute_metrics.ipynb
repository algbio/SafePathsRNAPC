{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you first need to run the notebook `experiments/run_experiments.ipynb` or obtain the data generated for such notebook in the corresponding folders\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump, load\n",
    "import networkx as nx\n",
    "\n",
    "## Load gene graphs\n",
    "\n",
    "f = open(f'../gene_graphs/vertices_inv.json', 'r')\n",
    "vertices_inv = load(f)\n",
    "f.close()\n",
    "vertices_inv = {int(k): tuple(v) for k,v in vertices_inv.items()}\n",
    "\n",
    "components = list()\n",
    "for i in range(19641): ## Number of gene_graphs\n",
    "    components.append(dict())\n",
    "    components[i]['graph'] = nx.read_edgelist(f'../gene_graphs/graphs/component_{i+1}.edgelist', delimiter=':', create_using=nx.DiGraph, nodetype=int)\n",
    "    components[i]['len'] = len(components[i]['graph'])\n",
    "    \n",
    "    f = open(f'../gene_graphs/sources/component_{i+1}.json', 'r')\n",
    "    components[i]['sources'] = set(load(f))\n",
    "    f.close()\n",
    "    \n",
    "    f = open(f'../gene_graphs/targets/component_{i+1}.json', 'r')\n",
    "    components[i]['targets'] = set(load(f))\n",
    "    f.close()\n",
    "    \n",
    "    f = open(f'../gene_graphs/vertex_constrains/component_{i+1}.json', 'r')\n",
    "    components[i]['vertex_constrains'] = set(load(f))\n",
    "    f.close()\n",
    "    \n",
    "    f = open(f'../gene_graphs/transcript_paths/component_{i+1}.json', 'r')\n",
    "    components[i]['transcript_paths'] = load(f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load experiments from json files\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        file = open(f'../safe_paths_json/component_{i+1}.json', \"r\")\n",
    "        dd = load(file)\n",
    "        component.update(dd)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the first or last vertex are internals of a unitig, extend them by one vertex, respectively\n",
    "def extend_unitig(G, in_neighbor, unitig):\n",
    "    first = unitig[0]\n",
    "    last = unitig[-1]\n",
    "    \n",
    "    if G.in_degree(first) == 1:\n",
    "        unitig = [in_neighbor[first]] + unitig\n",
    "    if G.out_degree(last) == 1:\n",
    "        unitig = unitig + list(G.neighbors(last))\n",
    "    \n",
    "    return unitig\n",
    "    \n",
    "## Given a DAG G, it computes maximal unitigs of G \n",
    "## that are safe paths of every ST-path cover\n",
    "def compute_maximal_safe_unitigs(G, S , T): \n",
    "    visited = dict(zip(G.nodes, len(G)*[False]))\n",
    "    normal_unitigs = list()\n",
    "    \n",
    "    for v in nx.topological_sort(G):\n",
    "        if not visited[v]:\n",
    "            visited[v] = True\n",
    "            \n",
    "            if G.out_degree(v) == 1 and G.in_degree(v) <= 1:\n",
    "                normal_unitig = [v] # We start a unitig\n",
    "                \n",
    "                t = list(G.neighbors(v))[0]\n",
    "                while G.in_degree(t) == 1 and G.out_degree(t) == 1: ## While internal vertex have in/outdegree=1, we append those vertices to the unitig\n",
    "                    visited[t] = True\n",
    "                    normal_unitig.append(t)\n",
    "                    t = list(G.neighbors(t))[0]\n",
    "                 \n",
    "                if G.in_degree(v) == 1 and G.out_degree(v) == 0:\n",
    "                    visited[t] = True\n",
    "                    normal_unitig.append(t)\n",
    "                \n",
    "                normal_unitigs.append(normal_unitig)\n",
    "                \n",
    "            elif G.out_degree(v) == 0:\n",
    "                normal_unitigs.append([v])\n",
    "      \n",
    "    in_neighbor = dict()\n",
    "    for u in G:\n",
    "        for v in list(G.neighbors(u)):\n",
    "            if G.in_degree(v) == 1:\n",
    "                in_neighbor[v] = u\n",
    "            \n",
    "        \n",
    "    normal_unitigs = list(\n",
    "        map(\n",
    "            lambda unitig: extend_unitig(G, in_neighbor, unitig),\n",
    "            normal_unitigs\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    normal_unitigs = list(filter(lambda unitig: len(unitig)>2, normal_unitigs))\n",
    "    \n",
    "    ## Now we cut them so, these unitigs are ST safe\n",
    "    ST_unitigs = list()\n",
    "    st = False\n",
    "    for normal_unitig in normal_unitigs:\n",
    "        last_s = 0\n",
    "        s = 0\n",
    "        for i, v in enumerate(normal_unitig):\n",
    "            \n",
    "            if v in S:\n",
    "                last_s = i\n",
    "                if st:\n",
    "                    st = False\n",
    "                    s = i\n",
    "                \n",
    "            if i == len(normal_unitig)-1:\n",
    "                ST_unitigs.append(normal_unitig[s: i+1])\n",
    "                s = last_s\n",
    "                \n",
    "            elif v in T:\n",
    "                if last_s != s:\n",
    "                    ST_unitigs.append(normal_unitig[s: i+1])\n",
    "                    s = last_s\n",
    "                elif i+1 < len(normal_unitig) and normal_unitig[i+1] in S:\n",
    "                    ST_unitigs.append(normal_unitig[s: i+1])\n",
    "                    s = i+1\n",
    "                    \n",
    "            st = v in T and v in S\n",
    "    \n",
    "    ST_unitigs =  list(filter(lambda unitig: len(unitig)>1, ST_unitigs))\n",
    "    return ST_unitigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_length(interval):\n",
    "    return interval[1]-interval[0]+1\n",
    "\n",
    "## It computes the base length of a transcript\n",
    "def base_length(contig, vertices_inv):\n",
    "    length = 0\n",
    "    for v in contig:\n",
    "        length += interval_length(vertices_inv[v])\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given a list of n base contigs and n improved contigs, this function returns\n",
    "## a list of l elements, where the i-th element is the difference of length between\n",
    "## the i-th base_contig and the longest improved_contig to which the base_contig\n",
    "## aligns completely, or 0 if not such improved contig exists\n",
    "## Two lists are returned, the first in vertex space and the second in base space\n",
    "def relative_improvement(base_contigs, improved_contigs, vertices_inv):\n",
    "    max_vertex_lengths = list()\n",
    "    max_base_lengths = list()\n",
    "    \n",
    "    base_contigs_staring_at = dict()\n",
    "    ## For every start vertex in a base_contig, put the corresponding base_contig in the dict\n",
    "    for c, base_contig in enumerate(base_contigs):\n",
    "        v = base_contig[0]\n",
    "        if base_contigs_staring_at.get(v, None) is None:\n",
    "            base_contigs_staring_at[v] = list()\n",
    "        base_contigs_staring_at[v].append(c)\n",
    "        \n",
    "        \n",
    "    max_vertex_lengths = [0]*len(base_contigs)\n",
    "    max_base_lengths = [0]*len(base_contigs)\n",
    "    \n",
    "    for improved_contig in improved_contigs:\n",
    "        for i,v in enumerate(improved_contig):\n",
    "            if base_contigs_staring_at.get(v, None) is not None:\n",
    "                for c in base_contigs_staring_at[v]:\n",
    "                    base_contig = base_contigs[c]\n",
    "                    if i+len(base_contig) <= len(improved_contig):\n",
    "                        aligns = True\n",
    "                        for j in range(len(base_contig)):\n",
    "                            if base_contig[j] != improved_contig[j+i]:\n",
    "                                aligns = False\n",
    "                                break\n",
    "                        if aligns:\n",
    "                            max_vertex_lengths[c] = max(max_vertex_lengths[c], len(improved_contig))\n",
    "                            max_base_lengths[c] = max(max_base_lengths[c], base_length(improved_contig, vertices_inv))\n",
    "                            \n",
    "        \n",
    "    \n",
    "    return max_vertex_lengths, max_base_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(components):\n",
    "    component['unitigs'] = compute_maximal_safe_unitigs(component['graph'], component['sources'], component['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter unitigs included inside other unitigs, needs relative_improvement function\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        \n",
    "        large, _ = relative_improvement(component['unitigs'], component['unitigs'] , vertices_inv)\n",
    "        lengths = list(map(lambda unitig: len(unitig), component['unitigs']))\n",
    "        \n",
    "        filter_unitigs = list()\n",
    "        for j in range(len(large)):\n",
    "            if large[j] <= lengths[j]:\n",
    "                filter_unitigs.append(component['unitigs'][j])\n",
    "        component['unitigs'] = filter_unitigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The e_size is computed for every transcript path, and for every base in this transcript path\n",
    "## For every transcript EVERY contig intersecting with that contig is considered\n",
    "def compute_e_size(transcript_paths, contigs, vertices_inv):\n",
    "    contigs_through = dict()\n",
    "    ## For every vertex in a contig, compute the contigs using that vertex and its index in the corresponding contig\n",
    "    for c, contig in enumerate(contigs):\n",
    "        for i, v in enumerate(contig):\n",
    "            if contigs_through.get(v, None) is None:\n",
    "                contigs_through[v] = list()\n",
    "            contigs_through[v].append((c,i))\n",
    "    \n",
    "    e_size_per_transcript_path = list()\n",
    "    e_size_per_transcript_path_vertex = list()\n",
    "    for transcript_path in transcript_paths:\n",
    "        \n",
    "        length = 0\n",
    "        e_sum = 0\n",
    "        e_sum_vertex = 0\n",
    "        for j, v in enumerate(transcript_path):\n",
    "            exon_length = interval_length(vertices_inv[v])\n",
    "            length += exon_length\n",
    "            \n",
    "            if contigs_through.get(v, None) is not None:\n",
    "                total_length_intersections = 0\n",
    "                total_length_intersections_vertex = 0\n",
    "                \n",
    "                for c,i in contigs_through[v]:\n",
    "                    \n",
    "                    contig = contigs[c]\n",
    "                    l_p = i\n",
    "                    while l_p >= 0 and (j-(i-l_p)) >= 0 and contig[l_p] == transcript_path[(j-(i-l_p))]:\n",
    "                        l_p -= 1\n",
    "                    \n",
    "                    l_p += 1\n",
    "                    \n",
    "                    r_p = i\n",
    "                    while r_p < len(contig) and (j-(i-r_p)) < len(transcript_path) and contig[r_p] == transcript_path[(j-(i-r_p))]:\n",
    "                        r_p += 1\n",
    "                    r_p -= 1\n",
    "                    \n",
    "                    intersection = contig[l_p:r_p+1]\n",
    "                    total_length_intersections += base_length(intersection, vertices_inv)\n",
    "                    total_length_intersections_vertex += len(intersection)\n",
    "                    \n",
    "                    \n",
    "                e_sum += exon_length*(total_length_intersections/len(contigs_through[v]))\n",
    "                e_sum_vertex += total_length_intersections_vertex/len(contigs_through[v])\n",
    "                    \n",
    "                \n",
    "        e_size_per_transcript_path.append(e_sum/length)\n",
    "        e_size_per_transcript_path_vertex.append(e_sum_vertex/len(transcript_path))\n",
    "    \n",
    "    return e_size_per_transcript_path_vertex, e_size_per_transcript_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns a list t_p of size len(contigs) such that \n",
    "## tp[i] is true iff contigs[i] aligns completely to some\n",
    "## transcript path\n",
    "def true_positives(transcript_paths, contigs, vertices_inv):\n",
    "    t_p = [False]*len(contigs)\n",
    "    \n",
    "    contigs_staring_at = dict()\n",
    "    ## For every start vertex in a contig, put the corresponding contig in the dict\n",
    "    for c, contig in enumerate(contigs):\n",
    "        v = contig[0]\n",
    "        if contigs_staring_at.get(v, None) is None:\n",
    "            contigs_staring_at[v] = list()\n",
    "        contigs_staring_at[v].append(c)\n",
    "    \n",
    "    \n",
    "    for transcript_path in transcript_paths:\n",
    "        \n",
    "        for i,v in enumerate(transcript_path):\n",
    "            if contigs_staring_at.get(v, None) is not None:\n",
    "                not_aligning_contigs = list()\n",
    "                \n",
    "                for c in contigs_staring_at[v]: ## All contigs evaluated start at some vertex of the transcript\n",
    "                    contig = contigs[c]\n",
    "                    if i+len(contig) <= len(transcript_path): ## They also end at some vertex of the contig\n",
    "                        aligns = True\n",
    "                        for j in range(len(contig)):\n",
    "                            if contig[j] != transcript_path[j+i]:\n",
    "                                aligns = False\n",
    "                                break\n",
    "                        if aligns:\n",
    "                            t_p[c] = True\n",
    "                        else:\n",
    "                            not_aligning_contigs.append(c)\n",
    "                    else:\n",
    "                        not_aligning_contigs.append(c)\n",
    "                \n",
    "                contigs_staring_at[v] = not_aligning_contigs\n",
    "    \n",
    "    \n",
    "    return t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns a list max_cov of length len(transcript_paths)\n",
    "## such that max_cov[i] is the maximum of (vertices, bases)\n",
    "## covered by any contig in contigs INTERSECTING to transcript_path[i]\n",
    "def max_covered_by_a_contig(transcript_paths, contigs, vertices_inv):\n",
    "    max_cov_vertex = list()\n",
    "    max_cov_bases = list()\n",
    "    \n",
    "    contigs_through = dict()\n",
    "    \n",
    "    ## For every vertex in a contig, compute the contigs using that vertex and its index in the corresponding contig\n",
    "    for c, contig in enumerate(contigs):\n",
    "        for i, v in enumerate(contig):\n",
    "            if contigs_through.get(v, None) is None:\n",
    "                contigs_through[v] = list()\n",
    "            contigs_through[v].append((c,i))\n",
    "            \n",
    "    \n",
    "    for transcript_path in transcript_paths:\n",
    "        max_bases = 0\n",
    "        max_vertex = 0\n",
    "        \n",
    "        for j,v in enumerate(transcript_path):\n",
    "            \n",
    "            \n",
    "            \n",
    "            if contigs_through.get(v, None) is not None:\n",
    "                \n",
    "                for c,i in contigs_through[v]:\n",
    "                    \n",
    "                    contig = contigs[c]\n",
    "                    l_p = i\n",
    "                    while l_p >= 0 and (j-(i-l_p)) >= 0 and contig[l_p] == transcript_path[(j-(i-l_p))]:\n",
    "                        l_p -= 1\n",
    "                    \n",
    "                    l_p += 1\n",
    "                    \n",
    "                    r_p = i\n",
    "                    while r_p < len(contig) and (j-(i-r_p)) < len(transcript_path) and contig[r_p] == transcript_path[(j-(i-r_p))]:\n",
    "                        r_p += 1\n",
    "                    r_p -= 1\n",
    "                    \n",
    "                    \n",
    "                    intersection = contig[l_p:r_p+1]\n",
    "                    length_intersection_vertex = len(intersection)\n",
    "                    length_intersection_bases = base_length(intersection, vertices_inv)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    max_bases = max(max_bases, length_intersection_bases)\n",
    "                    max_vertex = max(max_vertex, length_intersection_vertex)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        max_cov_bases.append(max_bases)\n",
    "        max_cov_vertex.append(max_vertex)\n",
    "    \n",
    "    return max_cov_vertex, max_cov_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute statistics for unitigs\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        transcript_paths = component['transcript_paths']\n",
    "        unitigs = component['unitigs']\n",
    "            \n",
    "        e_sizes_vertex, e_size_bases = compute_e_size(transcript_paths, unitigs, vertices_inv)\n",
    "        \n",
    "        unitigs_lengths = list(map(lambda unitigs: float(base_length(unitigs, vertices_inv)) , unitigs))\n",
    "            \n",
    "        max_cov_vertex, max_cov_bases = max_covered_by_a_contig(transcript_paths, unitigs, vertices_inv)\n",
    "            \n",
    "        tps = true_positives(transcript_paths,  unitigs, vertices_inv)\n",
    "        true_pos = list()\n",
    "        false_pos = list()\n",
    "        for i, tp in enumerate(tps):\n",
    "            unitig = unitigs[i]\n",
    "            if tp:\n",
    "                true_pos.append(unitig)\n",
    "            else:\n",
    "                false_pos.append(unitig)\n",
    "\n",
    "        component['e_sizes_vertex'] = e_sizes_vertex\n",
    "        component['e_size_bases'] = e_size_bases\n",
    "        \n",
    "        \n",
    "        component['unitigs_lengths'] = unitigs_lengths\n",
    "        \n",
    "        component['max_cov_vertex'] = max_cov_vertex\n",
    "        component['max_cov_bases'] = max_cov_bases\n",
    "        \n",
    "        component['true_positives'] = true_pos\n",
    "        component['false_positives'] = false_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(l):\n",
    "    return list(map(lambda e: float(e), l))\n",
    "\n",
    "\n",
    "## Compute evaluation metrics by gene graph\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        n = len(component['graph'])\n",
    "        uni = component['unitigs']\n",
    "        transcript_paths = component['transcript_paths']\n",
    "        experiments = component['experiments']\n",
    "        for l in experiments:\n",
    "            experiment = experiments[l]\n",
    "            \n",
    "            safe_paths = experiment['safe_paths']\n",
    "            \n",
    "            e_sizes_vertex, e_size_bases = compute_e_size(transcript_paths, safe_paths, vertices_inv)\n",
    "            \n",
    "            safe_paths_lengths = list(map(lambda contig: float(base_length(contig, vertices_inv)) , safe_paths))\n",
    "            \n",
    "            max_cov_vertex, max_cov_bases = max_covered_by_a_contig(transcript_paths, safe_paths, vertices_inv)\n",
    "            \n",
    "            improvement_vertex, improvement_base = relative_improvement(uni, safe_paths, vertices_inv)\n",
    "            \n",
    "            tps = true_positives(transcript_paths,  safe_paths, vertices_inv)\n",
    "            true_pos = list()\n",
    "            false_pos = list()\n",
    "            for i, tp in enumerate(tps):\n",
    "                safe_path = safe_paths[i]\n",
    "                if tp:\n",
    "                    true_pos.append(safe_path)\n",
    "                else:\n",
    "                    false_pos.append(safe_path)\n",
    "            \n",
    "            experiment['e_sizes_vertex'] = to_float(e_sizes_vertex)\n",
    "            experiment['e_size_bases'] = to_float(e_size_bases)\n",
    "            \n",
    "            experiment['safe_paths_lengths'] = safe_paths_lengths\n",
    "            \n",
    "            experiment['max_cov_vertex'] = to_float(max_cov_vertex)\n",
    "            experiment['max_cov_bases'] = to_float(max_cov_bases)\n",
    "            \n",
    "            experiment['true_positives'] = true_pos\n",
    "            experiment['false_positives'] = false_pos\n",
    "            \n",
    "            experiment['impr_vertex'] = improvement_vertex\n",
    "            experiment['impr_base'] = improvement_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "## Store these results to a file in json format\n",
    "\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        d = dict()\n",
    "        d['width'] = component['width']\n",
    "        d['number_of_transcripts'] = len(component['transcript_paths'])\n",
    "        d['experiments'] = component['experiments']\n",
    "        d['experiments_two_finger'] = component['experiments_two_finger']\n",
    "        d['experiments_unoptimized'] = component['experiments_unoptimized']\n",
    "        d['experiments_heuristic'] = component['experiments_heuristic']\n",
    "        \n",
    "        d['unitigs'] = component['unitigs']\n",
    "        d['e_sizes_vertex'] = component['e_sizes_vertex']\n",
    "        d['e_size_bases'] = component['e_size_bases']\n",
    "        d['unitigs_lengths'] = component['unitigs_lengths']\n",
    "        d['max_cov_vertex'] = component['max_cov_vertex']\n",
    "        d['max_cov_bases'] = component['max_cov_bases']\n",
    "        d['true_positives'] = component['true_positives']\n",
    "        d['false_positives'] = component['false_positives']\n",
    "        \n",
    "        file = open(f'../safe_paths_json/component_{i+1}.json' , 'w')\n",
    "        dump(d, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
