{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import of used libraries\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First index of chromosomes if put in the order 1...22, artificial starts\n",
    "offset = dict()\n",
    "offset[1] = 0\n",
    "offset[2] = 594993109\n",
    "offset[3] = 1284990015\n",
    "offset[4] = 1781411553\n",
    "offset[5] = 2563188567\n",
    "offset[6] = 3365350636\n",
    "offset[7] = 4018079240\n",
    "offset[8] = 4769821308\n",
    "offset[9] = 5601432115\n",
    "offset[10] = 6217887608\n",
    "offset[11] = 6963402469\n",
    "offset[12] = 7637937020\n",
    "offset[13] = 8148756134\n",
    "offset[14] = 8859430820\n",
    "offset[15] = 9573571679\n",
    "offset[16] = 10140546609\n",
    "offset[17] = 10759349778\n",
    "offset[18] = 11481154784\n",
    "offset[19] = 11955518679\n",
    "offset[20] = 12693015552\n",
    "offset[21] = 13444617899\n",
    "offset[22] = 14084088008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in .bed format\n",
    "d = pd.read_csv(f'../all.bed', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bed_format(d, offset):\n",
    "    ## Get positive strand\n",
    "    d_pos = d[d[5] == '+']\n",
    "    \n",
    "    ## Drop unnecessary columns and name the required\n",
    "    d_pos = d_pos.drop([2, 4, 5, 6, 7, 8, 9], axis=1)\n",
    "    d_pos.columns = ['chromosome', 'transcript_start', 'transcript_id', 'exon_sizes', 'exon_starts']\n",
    "    \n",
    "    ##Compute absolute genome positions for transcripts\n",
    "    d_pos.transcript_start = list(map(lambda x: x[0]+offset[x[1]], list(zip(d_pos.transcript_start, d_pos.chromosome))))\n",
    "    \n",
    "    ##Maybe something different for negative strand?\n",
    "\n",
    "    #Convert exon_sizes and exon_starts into list of numbers\n",
    "    d_pos.exon_sizes = list(map(lambda x: list(map(numpy.int64 , x[:-1].split(','))) ,d_pos.exon_sizes))\n",
    "    d_pos.exon_starts = list(map(lambda x: list(map(numpy.int64 , x[:-1].split(','))) ,d_pos.exon_starts))\n",
    "    \n",
    "    return d_pos\n",
    "\n",
    "d_pos = preprocess_bed_format(d, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function transforms processed table into a list of exon_endpoints\n",
    "def get_exon_endpoint_positions(d):\n",
    "    exons_endpoints = list()\n",
    "    for index, transcript in d.iterrows():\n",
    "        start = transcript.transcript_start\n",
    "        transcript_exons_endpoints = list()\n",
    "        for i in range(len(transcript.exon_starts)):\n",
    "            exon_start = start+transcript.exon_starts[i]\n",
    "            transcript_exons_endpoints.append({'position': exon_start, 'transcript_index': index, 'exon_index': i, 'start_point': True})\n",
    "            transcript_exons_endpoints.append({'position': exon_start+transcript.exon_sizes[i]-1, 'transcript_index': index, 'exon_index': i, 'start_point': False})\n",
    "        exons_endpoints.append(transcript_exons_endpoints)\n",
    "    return [item for sublist in exons_endpoints for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtains a list of all exons and sorts them according to its position and in case of ties it puts first the starting positions\n",
    "exon_endpoint_pos_list = get_exon_endpoint_positions(d_pos)\n",
    "exon_endpoint_pos_list.sort(key=lambda x: [x['position'], not(x['start_point'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the exons when overlapping with another\n",
    "active_exons = dict()\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        exon_endpoint['starting_points'] = list()\n",
    "        active_exons[(exon_endpoint['transcript_index'], exon_endpoint['exon_index'])] = exon_endpoint\n",
    "        \n",
    "        for key in active_exons:\n",
    "            active_exons[key]['starting_points'].append(exon_endpoint['position'])\n",
    "    else:\n",
    "        for key in active_exons:\n",
    "            active_exons[key]['starting_points'].append(exon_endpoint['position']+1)\n",
    "        del active_exons[(exon_endpoint['transcript_index'], exon_endpoint['exon_index'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute pseudo-exons\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        exon_endpoint['pseudo_exons'] = list()\n",
    "        previous_value = exon_endpoint['starting_points'][0]\n",
    "        for i in range(1, len(exon_endpoint['starting_points'])):\n",
    "            if previous_value != exon_endpoint['starting_points'][i]:\n",
    "                exon_endpoint['pseudo_exons'].append((previous_value, exon_endpoint['starting_points'][i]-1))\n",
    "                previous_value = exon_endpoint['starting_points'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group pseudo_exons by transcript (assumption: exons of a transcript do not overlap)\n",
    "transcripts = dict()\n",
    "for exon_endpoint in exon_endpoint_pos_list:\n",
    "    if exon_endpoint['start_point']:\n",
    "        if transcripts.get(exon_endpoint['transcript_index'], None) is None:\n",
    "            transcripts[exon_endpoint['transcript_index']] = {'pseudo_exons': list()}\n",
    "        for pseudo_exon in exon_endpoint['pseudo_exons']:\n",
    "            transcripts[exon_endpoint['transcript_index']]['pseudo_exons'].append(pseudo_exon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the pseudo_exons column\n",
    "pseudo_exons_list = list()\n",
    "for key in transcripts:\n",
    "    transcripts[key].update({'id': key})\n",
    "    pseudo_exons_list.append(transcripts[key])\n",
    "pseudo_exons_list.sort(key= lambda x: x['id'])\n",
    "\n",
    "pseudo_exons_column = list(map(lambda x: x['pseudo_exons'] ,pseudo_exons_list))\n",
    "d_pos['pseudo_exons'] = pseudo_exons_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vertex set\n",
    "vertices = dict() # Given a exon presudo_exon (x,y) return its id\n",
    "vertices_inv = dict() # Given an id returns the corresponding exon (x,y)\n",
    "next_id = 0\n",
    "for pseudo_exons in pseudo_exons_column:\n",
    "    for pseudo_exon in pseudo_exons:\n",
    "        if vertices.get(pseudo_exon, None) is None:\n",
    "            vertices[pseudo_exon] = next_id\n",
    "            vertices_inv[next_id] = pseudo_exon\n",
    "            next_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build edge set, source and target vertices\n",
    "# It also builds the transcript paths starting at every source (This could generate multiedges in the graph)\n",
    "\n",
    "## These dicts are indexed by the pair [exon_start, exon_end]\n",
    "transcript_paths = dict()\n",
    "sources = dict()\n",
    "targets = dict()\n",
    "\n",
    "## The keys are the edges and the edges and the value the corresponding id\n",
    "edges = dict()\n",
    "next_id = 0 \n",
    "for index, row in d_pos.iterrows():\n",
    "    pseudo_exons = row['pseudo_exons']\n",
    "    if sources.get(pseudo_exons[0], None) is None:\n",
    "        sources[pseudo_exons[0]] = set()\n",
    "    if targets.get(pseudo_exons[-1], None) is None:\n",
    "        targets[pseudo_exons[-1]] = set()\n",
    "    \n",
    "    if transcript_paths.get(pseudo_exons[0], None) is None:\n",
    "        transcript_paths[pseudo_exons[0]] = list()\n",
    "    \n",
    "    \n",
    "    sources[pseudo_exons[0]].add(index)\n",
    "    targets[pseudo_exons[-1]].add(index)\n",
    "    \n",
    "    \n",
    "    transcript_path = [vertices[pseudo_exons[0]]]\n",
    "    ## Consecutive pseudo exons in pseudo_exons are linked by an edge\n",
    "    for i in range(len(pseudo_exons)-1):\n",
    "        current_pe = pseudo_exons[i]\n",
    "        next_pe = pseudo_exons[i+1]\n",
    "        edge = (vertices[current_pe], vertices[next_pe])\n",
    "        if edges.get(edge, None) is None:\n",
    "            edges[edge] = next_id\n",
    "            next_id += 1\n",
    "        transcript_path.append(vertices[next_pe])\n",
    "    \n",
    "    transcript_paths[pseudo_exons[0]].append(transcript_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph to find weakly connected components, and also \n",
    "# computes len, sources, target and transcipt paths for each\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(len(vertices)))\n",
    "G.add_edges_from(edges.keys())\n",
    "\n",
    "components = list()\n",
    "for component_v in nx.weakly_connected_components(G):\n",
    "    component_dict = {'graph':G.subgraph(component_v)}\n",
    "    component_dict['len'] = len(component_dict['graph'])\n",
    "    sources_component = set()\n",
    "    targets_component = set()\n",
    "    transcript_paths_component = list()\n",
    "    \n",
    "    for vertex in component_v:\n",
    "        interval = vertices_inv[vertex]\n",
    "        if sources.get(interval, None) is not None:\n",
    "            sources_component.add(vertex)\n",
    "            transcript_paths_component += transcript_paths[interval]\n",
    "            \n",
    "        if targets.get(interval, None) is not None:\n",
    "            targets_component.add(vertex)\n",
    "    component_dict['sources'] = sources_component\n",
    "    component_dict['targets'] = targets_component\n",
    "    component_dict['transcript_paths'] = transcript_paths_component\n",
    "    component_dict['vertex_constrains'] = set(component_v)\n",
    "    \n",
    "    components.append(component_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "## It stores the networkx graph, the transcript paths, the sources and targets\n",
    "def store_components_to_files(component, i):\n",
    "    gene_graph = component['graph']\n",
    "    nx.write_edgelist(gene_graph, path=f'../gene_graphs/graphs/component_{i+1}.edgelist', delimiter=':')\n",
    "    \n",
    "    transcript_paths = component['transcript_paths']\n",
    "    f = open(f'../gene_graphs/transcript_paths/component_{i+1}.json', 'w')\n",
    "    dump(transcript_paths, f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    sources = list(component['sources'])\n",
    "    f = open(f'../gene_graphs/sources/component_{i+1}.json', 'w')\n",
    "    dump(sources, f)\n",
    "    f.close()\n",
    "    \n",
    "    targets = list(component['targets'])\n",
    "    f = open(f'../gene_graphs/targets/component_{i+1}.json', 'w')\n",
    "    dump(targets, f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    vertex_constrains = list(component['vertex_constrains'])\n",
    "    f = open(f'../gene_graphs/vertex_constrains/component_{i+1}.json', 'w')\n",
    "    dump(vertex_constrains, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store components/gene_graphs to files\n",
    "for i,component in enumerate(components):\n",
    "    store_components_to_files(component, i)\n",
    "\n",
    "\n",
    "## Store vertices_inv: id --> (genome_pos, genome_pos)\n",
    "vertices_inv = {key: (int(x), int(y)) for key, (x, y) in vertices_inv.items()}\n",
    "f = open(f'../gene_graphs/vertices_inv.json', 'w')\n",
    "dump(vertices_inv, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It stores the corresponding component as lemon graph format file in filename\n",
    "## Vertex mappings original_id (networkx's id), is_source (if it is a source), is_target (if it is a target)\n",
    "## and is_vertex_constrain (if it is a vertex constrain) are included in the file\n",
    "def store_to_file_in_lemon_format_with_mappings(component, filename):\n",
    "    G = component['graph']\n",
    "    sources = component['sources']\n",
    "    targets = component['targets']\n",
    "    constrains = component['vertex_constrains']\n",
    "    \n",
    "    file = open(filename, 'w')\n",
    "    file.write(\"@nodes\\n\")\n",
    "    file.write(\"label\\toriginal_id\\tis_source\\tis_target\\tis_vertex_constrain\\t\\n\")\n",
    "    for vertex in G.nodes:\n",
    "        file.write(str(vertex)+\"\\t\"+str(vertex)+\"\\t\"+str(1 if vertex in sources else 0)+\"\\t\"+str(1 if vertex in targets else 0)+\"\\t\"+str(1 if vertex in constrains else 0)+\"\\t\\n\")\n",
    "    file.write(\"@arcs\\n\")\n",
    "    file.write(\"\\t\\tlabel\\t\\n\")\n",
    "    for i, edge in enumerate(G.edges):\n",
    "        file.write(str(edge[0])+\"\\t\"+str(edge[1])+\"\\t\"+ str(i) + \"\\t\\n\")  \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store components with more than to vertices to the corresponding lemon graph format\n",
    "## Can be skipped if already computed\n",
    "for i, component in enumerate(components):\n",
    "    if component['len'] > 2:\n",
    "        store_to_file_in_lemon_format_with_mappings(component,'../lgf/component_'+str(i+1)+'.lgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
